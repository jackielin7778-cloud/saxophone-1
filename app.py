import streamlit as st
import pandas as pd
import time
import random
import re
import os
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from io import BytesIO

# --- 1. é é¢åˆå§‹åŒ– ---
st.set_page_config(page_title="ğŸ· è–©å…‹æ–¯é¢¨å¹å˜´å¸‚èª¿ç³»çµ±", layout="wide")

if 'url_list' not in st.session_state:
    st.session_state.url_list = []

def get_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    chrome_options.add_argument(f"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36")
    
    for path in ["/usr/bin/chromium", "/usr/bin/chromium-browser"]:
        if os.path.exists(path):
            chrome_options.binary_location = path
            break
            
    service = Service("/usr/bin/chromedriver") if os.path.exists("/usr/bin/chromedriver") else Service()
    driver = webdriver.Chrome(service=service, options=chrome_options)
    driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
        "source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
    })
    return driver

def scrape_engine(urls):
    all_data = []
    log_placeholder = st.empty()
    logs = []

    def log(msg):
        logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")
        log_placeholder.code("\n".join(logs[-10:]))

    try:
        driver = get_driver()
        log("ğŸš€ ç€è¦½å™¨å•Ÿå‹•æˆåŠŸã€‚")
    except Exception as e:
        log(f"âŒ é©…å‹•ç•°å¸¸: {str(e)}")
        return pd.DataFrame()

    progress_bar = st.progress(0)
    for index, url in enumerate(urls):
        try:
            log(f"ğŸ” æ­£åœ¨æŠ“å– ({index+1}/{len(urls)})...")
            driver.get(url)
            
            load_time = random.uniform(5, 7)
            time.sleep(load_time)
            
            title = driver.title
            log(f"ğŸ“„ æ¨™é¡Œ: {title[:30]}...")

            platform = "è¦çš®" if "shopee" in url else "Yahooæ‹è³£"
            seller = "æœªçŸ¥è³£å®¶"
            price = "å°šæœªæ“·å–"
            
            # --- å¼·åŒ–è§£æé‚è¼¯ ---
            if platform == "Yahooæ‹è³£":
                # å˜—è©¦å¤šç¨®å¯èƒ½çš„è³£å®¶åç¨± CSS
                seller_selectors = [
                    'a[data-curst]', 
                    '.yui3-u-1 .name', 
                    'div[class*="SellerName"]', 
                    '.seller-name',
                    'span[class*="SellerName"]'
                ]
                for selector in seller_selectors:
                    els = driver.find_elements(By.CSS_SELECTOR, selector)
                    if els and els[0].text.strip():
                        seller = els[0].text.strip()
                        break
                
                # å˜—è©¦æŠ“å–åƒ¹æ ¼ (Yahoo çš„åƒ¹æ ¼é€šå¸¸åœ¨ç‰¹å®šçš„ class æˆ–åŒ…å« $ çš„å­—ä¸²)
                price_selectors = ['.price', '.product-price', 'span[class*="Price"]']
                for selector in price_selectors:
                    p_els = driver.find_elements(By.CSS_SELECTOR, selector)
                    if p_els:
                        price = p_els[0].text.strip()
                        break
                if price == "å°šæœªæ“·å–":
                    # æ­£å‰‡è¡¨é”å¼ä¿åº•æŠ“å–åƒ¹æ ¼
                    price_match = re.search(r'\$\s*[0-9,]+', driver.page_source)
                    if price_match: price = price_match.group()

            elif platform == "è¦çš®":
                s_els = driver.find_elements(By.CSS_SELECTOR, 'span.V67tSj, ._23_19X, .official-shop-label__name')
                if s_els: seller = s_els[0].text
                p_match = re.search(r'\$\s*[0-9,]+', driver.page_source)
                if p_match: price = p_match.group()

            # --- æ¨‚å™¨åˆ¤å®š ---
            content = driver.page_source.lower()
            instrument = "å…¶ä»–/é€šç”¨"
            if "alto" in content or "ä¸­éŸ³" in content: instrument = "ä¸­éŸ³Alto"
            elif "tenor" in content or "æ¬¡ä¸­éŸ³" in content: instrument = "æ¬¡ä¸­éŸ³Tenor"
            elif "soprano" in content or "é«˜éŸ³" in content: instrument = "é«˜éŸ³Soprano"

            all_data.append({
                "è³£æ–¹åç¨±": seller,
                "é©ç”¨æ¨‚å™¨": instrument,
                "å”®åƒ¹": price,
                "ä¾†æºå¹³å°": platform,
                "å•†å“ç¶²å€": url
            })
            log(f"âœ… è§£æå®Œæˆ: {seller} / {price}")

        except Exception as e:
            log(f"âŒ éŒ¯èª¤: {str(e)}")
        
        progress_bar.progress((index + 1) / len(urls))

    driver.quit()
    return pd.DataFrame(all_data)

# --- 2. UI ä»‹é¢ ---
st.title("ğŸ· è–©å…‹æ–¯é¢¨å¹å˜´å¸‚å ´èª¿æŸ¥ç³»çµ±")

url_input = st.text_area("è«‹è¼¸å…¥ç¶²å€ï¼š", height=100)
if st.button("â• æ›´æ–°æ¸…å–®"):
    st.session_state.url_list = [u.strip() for u in url_input.split("\n") if u.strip()]

if st.session_state.url_list:
    if st.button("ğŸš€ é–‹å§‹å…¨æ•¸æ‹”å›"):
        df = scrape_engine(st.session_state.url_list)
        if not df.empty:
            st.session_state.df_results = df
            st.dataframe(df)

if 'df_results' in st.session_state:
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        st.session_state.df_results.to_excel(writer, index=False)
    st.download_button("ğŸ“¥ ä¸‹è¼‰ Excel èª¿æŸ¥å ±å‘Š", output.getvalue(), "sax_report.xlsx")
