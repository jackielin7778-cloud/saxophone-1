import streamlit as st
import pandas as pd
import time
import random
import re
import os
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from io import BytesIO

# --- 1. é é¢é…ç½® ---
st.set_page_config(page_title="ğŸ· è–©å…‹æ–¯é¢¨å¹å˜´æœå°‹æ‹”å› (é è¨­ç¶²å€ç‰ˆ)", layout="wide")

def get_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    
    # éš¨æ©Ÿè¦–çª—å¤§å°èˆ‡ UA å½è£
    chrome_options.add_argument(f"--window-size={random.randint(1200, 1920)},{random.randint(800, 1080)}")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    
    ua = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
    chrome_options.add_argument(f"user-agent={ua}")
    
    for path in ["/usr/bin/chromium", "/usr/bin/chromium-browser"]:
        if os.path.exists(path):
            chrome_options.binary_location = path
            break
            
    service = Service("/usr/bin/chromedriver") if os.path.exists("/usr/bin/chromedriver") else Service()
    driver = webdriver.Chrome(service=service, options=chrome_options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    return driver

def scrape_search_enhanced(url):
    all_items = []
    log_placeholder = st.empty()
    logs = []

    def log(msg):
        logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")
        log_placeholder.code("\n".join(logs[-10:]))

    try:
        driver = get_driver()
        log("ğŸ•µï¸ èª¿æŸ¥å“¡å·²å°±ä½ï¼Œæ­£åœ¨åŸ·è¡Œæ½›å…¥è¡Œå‹•...")
        driver.execute_cdp_cmd('Network.setExtraHTTPHeaders', {'headers': {'Referer': 'https://www.google.com/'}})
        driver.get(url)
        
        # æ¨¡æ“¬äººé¡æ»¾å‹•
        for i in range(3):
            driver.execute_script(f"window.scrollBy(0, {random.randint(500, 800)});")
            time.sleep(random.uniform(2, 3))

        log(f"ğŸ“„ åµæ¸¬æ¨™é¡Œ: {driver.title}")
        
        # æš´åŠ›æƒææ‰€æœ‰å•†å“å¡Š
        elements = driver.find_elements(By.XPATH, "//li | //div[contains(@class, 'item')]")
        brand_list = ["Selmer", "Vandoren", "Yanagisawa", "Meyer", "Yamaha", "Otto Link", "Beechler"]
        
        for el in elements:
            try:
                txt = el.text.strip().replace("\n", " ")
                if "$" in txt and len(txt) > 20:
                    p_match = re.search(r'\$\s*[0-9,]+', txt)
                    price = p_match.group() if p_match else "N/A"
                    title = txt[:80].strip()
                    
                    brand = "å…¶ä»–"
                    for b in brand_list:
                        if b.lower() in title.lower():
                            brand = b
                            break
                    
                    instrument = "å…¶ä»–"
                    if "alto" in title.lower() or "ä¸­éŸ³" in title.lower(): instrument = "ä¸­éŸ³Alto"
                    elif "tenor" in title.lower() or "æ¬¡ä¸­éŸ³" in title.lower(): instrument = "æ¬¡ä¸­éŸ³Tenor"

                    all_items.append({
                        "å“ç‰Œ": brand,
                        "å•†å“æ¨™é¡Œ": title,
                        "é©ç”¨æ¨‚å™¨": instrument,
                        "å”®åƒ¹": price
                    })
            except: continue

        df = pd.DataFrame(all_items).drop_duplicates(subset=['å•†å“æ¨™é¡Œ', 'å”®åƒ¹'])
        log(f"âœ… æˆåŠŸæ‹”å› {len(df)} ç­†æ•¸æ“š")
        driver.quit()
        return df
    except Exception as e:
        log(f"âŒ ç•°å¸¸: {str(e)}")
        return pd.DataFrame()

# --- 2. UI ä»‹é¢ ---
st.title("ğŸ· è–©å…‹æ–¯é¢¨å¹å˜´å¸‚èª¿å·¥å…·")

# è¨­å®šé è¨­ç¶²å€
default_url = "https://tw.bid.yahoo.com/search/auction/product?p=%E8%96%A9%E5%85%8B%E6%96%AF%E9%A2%A8%E5%90%B9%E5%98%B4"
search_url = st.text_input("è¼¸å…¥ Yahoo æœå°‹çµæœç¶²å€ï¼š", value=default_url)

if st.button("ğŸš€ åŸ·è¡Œæ½›å…¥èª¿æŸ¥"):
    if search_url:
        results = scrape_search_enhanced(search_url)
        if not results.empty:
            st.session_state.final_df = results
            st.dataframe(results, use_container_width=True)
        else:
            st.error("èª¿æŸ¥çµæœç‚º 0ã€‚é€™ä»£è¡¨é›²ç«¯ IP ç›®å‰ä»è¢«å°é–ä¸­ï¼Œè«‹éæ®µæ™‚é–“å†è©¦ã€‚")

if 'final_df' in st.session_state:
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        st.session_state.final_df.to_excel(writer, index=False)
    st.download_button("ğŸ“¥ ä¸‹è¼‰ Excel èª¿æŸ¥å ±å‘Š", output.getvalue(), "sax_report.xlsx")
