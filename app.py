import streamlit as st
import pandas as pd
import time
import random
import os
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from io import BytesIO

# --- é é¢é…ç½® ---
st.set_page_config(page_title="ğŸ· è–©å…‹æ–¯é¢¨å¹å˜´æœå°‹å…¨æ•¸æ‹”å›", layout="wide")

def get_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    
    # æ³¨å…¥æ›´çœŸå¯¦çš„å½è£
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36")
    
    for path in ["/usr/bin/chromium", "/usr/bin/chromium-browser"]:
        if os.path.exists(path):
            chrome_options.binary_location = path
            break
            
    service = Service("/usr/bin/chromedriver") if os.path.exists("/usr/bin/chromedriver") else Service()
    driver = webdriver.Chrome(service=service, options=chrome_options)
    
    # é—œéµï¼šæŠ¹é™¤ Selenium æ¨™è¨˜
    driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
        "source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
    })
    return driver

def scrape_search_page(url):
    all_items = []
    log_placeholder = st.empty()
    logs = []

    def log(msg):
        logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")
        log_placeholder.code("\n".join(logs[-10:]))

    try:
        driver = get_driver()
        log("ğŸš€ ç€è¦½å™¨å·²å½è£å®Œæˆï¼Œæ­£åœ¨é€²å…¥æœå°‹é é¢...")
        driver.get(url)
        
        # æ»¾å‹•å¹¾æ¬¡ä»¥è§¸ç™¼å‹•æ…‹åŠ è¼‰
        for _ in range(3):
            driver.execute_script("window.scrollBy(0, 800);")
            time.sleep(2)
        
        log(f"ğŸ“„ ç¶²é æ¨™é¡Œ: {driver.title}")

        # --- 2026 Yahoo æ‹è³£æœ€æ–°å¤šé‡æ¢é‡ ---
        # å˜—è©¦æ‰¾å‡ºæ‰€æœ‰å¯èƒ½çš„å•†å“å®¹å™¨æ¨™ç±¤
        selectors = [
            'ul[class*="GeneralList"] li', 
            'div[class*="BaseItem"]',
            'li[data-item-id]',
            '.sc-762bc2d0-0' # Yahoo æ‹è³£å¸¸ç”¨çš„å‹•æ…‹ class
        ]
        
        items = []
        for s in selectors:
            items = driver.find_elements(By.CSS_SELECTOR, s)
            if len(items) > 5: # å¦‚æœæŠ“åˆ°è¶…é 5 å€‹ï¼Œä»£è¡¨é€™å€‹ selector æ˜¯å°çš„
                log(f"ğŸ¯ ä½¿ç”¨æ¢é‡ [{s}] æˆåŠŸæŠ“å–æ•¸æ“š")
                break
        
        if not items:
            log("âš ï¸ è­¦å‘Šï¼šç„¡æ³•è‡ªå‹•è­˜åˆ¥å•†å“å€å¡Šï¼Œå˜—è©¦æŠ“å–åŸå§‹é é¢ç‰¹å¾µ...")
            # ä¿åº•æ–¹æ¡ˆï¼šæŠ“å–æ‰€æœ‰åŒ…å«åƒ¹æ ¼ç¬¦è™Ÿçš„å€å¡Š
            items = driver.find_elements(By.XPATH, "//*[contains(text(), '$')]")

        log(f"ğŸ“¦ åµæ¸¬åˆ° {len(items)} å€‹æ½›åœ¨å•†å“å€å¡Š")

        for item in items:
            try:
                # ä½¿ç”¨ç›¸å°è·¯å¾‘æŠ“å–å…§å®¹ï¼Œé¿å…çµæ§‹è®Šå‹•å°è‡´å´©æ½°
                text_content = item.text.replace("\n", " ")
                if "$" not in text_content: continue
                
                # æŠ“å–åƒ¹æ ¼ (æ­£å‰‡è¡¨é”å¼)
                price_match = re.search(r'\$\s*[0-9,]+', text_content)
                price = price_match.group() if price_match else "N/A"
                
                # æŠ“å–æ¨™é¡Œèˆ‡åˆ¤æ–·æ¨‚å™¨
                title = text_content[:60] # å–å‰ 60 å­—ç•¶ä½œæ¨™é¡Œ
                instrument = "å…¶ä»–"
                t_lower = title.lower()
                if "alto" in t_lower or "ä¸­éŸ³" in t_lower: instrument = "ä¸­éŸ³Alto"
                elif "tenor" in t_lower or "æ¬¡ä¸­éŸ³" in t_lower: instrument = "æ¬¡ä¸­éŸ³Tenor"
                elif "soprano" in t_lower or "é«˜éŸ³" in t_lower: instrument = "é«˜éŸ³Soprano"

                # å˜—è©¦æ‰¾è³£å®¶ (é€šå¸¸åœ¨æ¨™é¡Œé™„è¿‘)
                seller = "å•†å®¶"
                # é€™è£¡åšä¸€å€‹ç°¡å–®çš„è³£å®¶åµæ¸¬ï¼Œè‹¥ item å…§æœ‰é€£çµï¼Œå˜—è©¦ç•¶ä½œè³£å®¶
                all_data = {
                    "è³£æ–¹åç¨±": seller,
                    "å•†å“è³‡è¨Š": title,
                    "é©ç”¨æ¨‚å™¨": instrument,
                    "å”®åƒ¹": price
                }
                all_items.append(all_data)
            except:
                continue

        # ç§»é™¤é‡è¤‡é …
        df = pd.DataFrame(all_items).drop_duplicates(subset=['å•†å“è³‡è¨Š', 'å”®åƒ¹'])
        log(f"âœ… æˆåŠŸæ‹”å› {len(df)} ç­†ä¸é‡è¤‡æ•¸æ“š")
        driver.quit()
        return df

    except Exception as e:
        log(f"âŒ ç™¼ç”Ÿç•°å¸¸: {str(e)}")
        return pd.DataFrame()

import re # è£œä¸Š regex æ¨¡çµ„

# --- UI ä»‹é¢ä¿æŒä¸è®Š ---
st.title("ğŸ· è–©å…‹æ–¯é¢¨å¹å˜´æœå°‹ã€Œå…¨æ•¸æ‹”å›ã€ç³»çµ±")
url_input = st.text_input("è¼¸å…¥ Yahoo æœå°‹çµæœç¶²å€ï¼š")
if st.button("ğŸš€ åŸ·è¡Œå…¨é æ‹”å›"):
    if url_input:
        res = scrape_search_page(url_input)
        if not res.empty:
            st.session_state.last_res = res
            st.dataframe(res)

if 'last_res' in st.session_state:
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        st.session_state.last_res.to_excel(writer, index=False)
    st.download_button("ğŸ“¥ ä¸‹è¼‰ Excel", output.getvalue(), "sax_list.xlsx")
