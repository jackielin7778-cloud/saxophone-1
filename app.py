import streamlit as st
import pandas as pd
import time
import random
import re
import os
import json
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from io import BytesIO

# --- é é¢é…ç½® ---
st.set_page_config(page_title="ğŸ· è–©å…‹æ–¯é¢¨å¹å˜´æœå°‹å…¨æ•¸æ‹”å›", layout="wide")

def get_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36")
    
    for path in ["/usr/bin/chromium", "/usr/bin/chromium-browser"]:
        if os.path.exists(path):
            chrome_options.binary_location = path
            break
            
    service = Service("/usr/bin/chromedriver") if os.path.exists("/usr/bin/chromedriver") else Service()
    driver = webdriver.Chrome(service=service, options=chrome_options)
    driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
        "source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
    })
    return driver

def identify_brand(title):
    brands = {
        "Selmer": ["selmer", "å¡çˆ¾ç‘ª", "s80", "s90"],
        "Vandoren": ["vandoren", "å‡¡å¤šå€«", "è¬å¤šæ—"],
        "Yanagisawa": ["yanagisawa", "æŸ³æ¾¤"],
        "Meyer": ["meyer"],
        "Yamaha": ["yamaha", "å±±è‘‰"],
        "JodyJazz": ["jodyjazz", "jody jazz"],
        "Otto Link": ["otto link", "ottolink"],
        "D'Addario": ["d'addario", "daddario"]
    }
    t_lower = title.lower()
    for brand, keywords in brands.items():
        if any(k in t_lower for k in keywords):
            return brand
    return "å…¶ä»–/è‡ªè£½"

def scrape_search_page(url):
    all_items = []
    log_placeholder = st.empty()
    logs = []

    def log(msg):
        logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")
        log_placeholder.code("\n".join(logs[-10:]))

    try:
        driver = get_driver()
        log("ğŸš€ åŸ·è¡Œçµ‚æ¥µè§£ææ–¹æ¡ˆ...")
        driver.get(url)
        time.sleep(8) # çµ¦äºˆå……è¶³æ™‚é–“åŠ è¼‰ JavaScript
        
        # --- æ ¸å¿ƒé‚è¼¯ï¼šå¾åŸå§‹ç¢¼æå– JSON æ•¸æ“š ---
        log("ğŸ” æ­£åœ¨æƒæç¶²é å¾Œå°æ•¸æ“šå€å¡Š...")
        page_source = driver.page_source
        
        # Yahoo æ‹è³£çš„å•†å“æ•¸æ“šé€šå¸¸å­˜åœ¨æ–¼ç¶²é ä¸­çš„ JSON æ ¼å¼
        # æˆ‘å€‘ç›´æ¥ç”¨æ­£å‰‡è¡¨é”å¼æŠ“å–åŒ…å«åƒ¹æ ¼èˆ‡æ¨™é¡Œçš„å­—ä¸²
        # å°‹æ‰¾æ¨¡å¼ï¼š{"title":"...","price":"..."}
        found_data = re.findall(r'{"title":"([^"]+)","ecPrice":"(\d+)"[^}]+"sellerName":"([^"]+)"', page_source)
        
        if not found_data:
            log("âš ï¸ JSON æå–å¤±æ•—ï¼Œåˆ‡æ›è‡³ DOM æ¨¹éæ­·æ¨¡å¼...")
            # å˜—è©¦æŠ“å–æ‰€æœ‰å…·æœ‰å•†å“ç‰¹å¾µçš„ç¯€é»
            items = driver.find_elements(By.XPATH, "//a[contains(@class, 'ItemName')] | //span[contains(@class, 'ItemName')]")
            log(f"ğŸ“¦ æ‰¾åˆ° {len(items)} å€‹æ½›åœ¨é€£çµæ¨™ç±¤")
            
            for item in items:
                try:
                    title = item.text
                    if len(title) < 5: continue
                    # å°‹æ‰¾è©²æ¨™ç±¤é™„è¿‘çš„åƒ¹æ ¼
                    parent = item.find_element(By.XPATH, "./ancestor::div[10]")
                    price_text = parent.text
                    price_match = re.search(r'\$\s*[0-9,]+', price_text)
                    price = price_match.group() if price_match else "N/A"
                    
                    all_items.append({
                        "å“ç‰Œ": identify_brand(title),
                        "è³£æ–¹åç¨±": "æœå°‹çµæœè³£å®¶",
                        "å•†å“è³‡è¨Š": title,
                        "é©ç”¨æ¨‚å™¨": "ä¸­éŸ³/æ¬¡ä¸­éŸ³" if "alto" in title.lower() or "ä¸­éŸ³" in title.lower() else "å…¶ä»–",
                        "å”®åƒ¹": price
                    })
                except: continue
        else:
            for title, price, seller in found_data:
                # å“ç‰Œèˆ‡æ¨‚å™¨åˆ¤å®š
                brand = identify_brand(title)
                t_lower = title.lower()
                instrument = "å…¶ä»–"
                if "alto" in t_lower or "ä¸­éŸ³" in t_lower: instrument = "ä¸­éŸ³Alto"
                elif "tenor" in t_lower or "æ¬¡ä¸­éŸ³" in t_lower: instrument = "æ¬¡ä¸­éŸ³Tenor"
                elif "soprano" in t_lower or "é«˜éŸ³" in t_lower: instrument = "é«˜éŸ³Soprano"
                
                all_items.append({
                    "å“ç‰Œ": brand,
                    "è³£æ–¹åç¨±": seller,
                    "å•†å“è³‡è¨Š": title,
                    "é©ç”¨æ¨‚å™¨": instrument,
                    "å”®åƒ¹": f"${price}"
                })

        df = pd.DataFrame(all_items).drop_duplicates(subset=['å•†å“è³‡è¨Š'])
        log(f"âœ… æˆåŠŸæ‹”å› {len(df)} ç­†æ•¸æ“š")
        driver.quit()
        return df

    except Exception as e:
        log(f"âŒ ç•°å¸¸: {str(e)}")
        return pd.DataFrame()

# UI ä»‹é¢èˆ‡ä¹‹å‰ç›¸åŒ
st.title("ğŸ· è–©å…‹æ–¯é¢¨å¹å˜´å¸‚èª¿ç³»çµ± (çµ‚æ¥µè§£æç‰ˆ)")
url_input = st.text_input("è¼¸å…¥ Yahoo æœå°‹çµæœç¶²å€ï¼š")
if st.button("ğŸš€ åŸ·è¡Œå…¨é æ‹”å›"):
    if url_input:
        res = scrape_search_page(url_input)
        if not res.empty:
            st.session_state.final_res = res
            st.dataframe(res)
        else:
            st.error("é€£çµ‚æ¥µæ–¹æ¡ˆä¹ŸæŠ“ä¸åˆ°æ•¸æ“šã€‚é€™æ¥µå¯èƒ½æ˜¯å› ç‚ºé›²ç«¯ IP è¢« Yahoo å¾¹åº•å±è”½ã€‚")

if 'final_res' in st.session_state:
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        st.session_state.final_res.to_excel(writer, index=False)
    st.download_button("ğŸ“¥ ä¸‹è¼‰ Excel å ±å‘Š", output.getvalue(), "sax_report.xlsx")
