import streamlit as st
import pandas as pd
import time
import random
import re
import os
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from io import BytesIO

# --- é é¢é…ç½® ---
st.set_page_config(page_title="ğŸ· è–©å…‹æ–¯é¢¨å¹å˜´å¸‚å ´èª¿æŸ¥ç³»çµ±", layout="wide")

if 'url_list' not in st.session_state:
    st.session_state.url_list = []

def get_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36")
    
    for path in ["/usr/bin/chromium", "/usr/bin/chromium-browser"]:
        if os.path.exists(path):
            chrome_options.binary_location = path
            break
            
    service = Service("/usr/bin/chromedriver") if os.path.exists("/usr/bin/chromedriver") else Service()
    return webdriver.Chrome(service=service, options=chrome_options)

def scrape_with_live_logs(urls):
    all_data = []
    
    # å»ºç«‹ä¸€å€‹å¯¦æ™‚æ—¥èªŒå®¹å™¨
    log_container = st.empty()
    logs = []

    def update_logs(msg):
        logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")
        log_container.code("\n".join(logs))

    from datetime import datetime
    
    update_logs("ğŸš€ å•Ÿå‹•ç€è¦½å™¨å¼•æ“...")
    try:
        driver = get_driver()
        update_logs("âœ… ç€è¦½å™¨å•Ÿå‹•æˆåŠŸã€‚")
    except Exception as e:
        update_logs(f"âŒ ç€è¦½å™¨å•Ÿå‹•å¤±æ•—: {str(e)}")
        return pd.DataFrame()

    progress_bar = st.progress(0)
    
    for index, url in enumerate(urls):
        update_logs(f"æ­£åœ¨å‰å¾€: {url}")
        try:
            driver.get(url)
            # éš¨æ©Ÿç­‰å¾…
            wait_time = random.uniform(5, 8)
            update_logs(f"ç­‰å¾…é é¢åŠ è¼‰ ({wait_time:.1f}s)...")
            time.sleep(wait_time)
            
            # å–å¾—ç¶²é æ¨™é¡Œ
            title = driver.title
            update_logs(f"ç¶²é æ¨™é¡Œ: {title}")
            
            # æª¢æŸ¥æ˜¯å¦è¢«é˜»æ“‹
            if "Robot Check" in title or "Access Denied" in title or "è«‹ç¢ºèªæ‚¨çš„é€£ç·š" in title:
                update_logs("âš ï¸ è­¦å‘Šï¼šåµæ¸¬åˆ°æ©Ÿå™¨äººé©—è­‰ï¼Œé é¢å·²è¢«å°é–ã€‚")
            
            # å˜—è©¦æŠ“å–å…§å®¹
            platform = "è¦çš®" if "shopee" in url else "Yahooæ‹è³£"
            seller = "æœªçŸ¥è³£å®¶"
            price = "0"
            
            if platform == "è¦çš®":
                # ç°¡å–®å°‹æ‰¾å¯èƒ½åŒ…å«è³£å®¶åç¨±çš„æ¨™ç±¤
                els = driver.find_elements(By.CSS_SELECTOR, 'span[class*="seller"], ._23_19X')
                if els: seller = els[0].text
                p_els = driver.find_elements(By.CSS_SELECTOR, 'div[class*="price"], .G277_P')
                if p_els: price = p_els[0].text
            else:
                els = driver.find_elements(By.CSS_SELECTOR, '.seller-name, .y-seller-name')
                if els: seller = els[0].text
            
            update_logs(f"è§£æçµæœ -> è³£å®¶: {seller}, åƒ¹æ ¼: {price}")
            
            # æ¨‚å™¨åˆ¤å®š
            content = driver.page_source.lower()
            instrument = "å…¶ä»–"
            if "alto" in content or "ä¸­éŸ³" in content: instrument = "ä¸­éŸ³Alto"
            elif "tenor" in content or "æ¬¡ä¸­éŸ³" in content: instrument = "æ¬¡ä¸­éŸ³Tenor"
            elif "soprano" in content or "é«˜éŸ³" in content: instrument = "é«˜éŸ³Soprano"

            all_data.append({
                "ä¾†æºå¹³å°": platform,
                "è³£æ–¹åç¨±": seller,
                "é©ç”¨æ¨‚å™¨": instrument,
                "å”®åƒ¹": price,
                "ç¶²å€": url
            })
            
        except Exception as e:
            update_logs(f"âŒ è§£æå‡ºéŒ¯: {str(e)}")
        
        progress_bar.progress((index + 1) / len(urls))
    
    driver.quit()
    update_logs("ğŸ çˆ¬èŸ²çµæŸï¼Œç€è¦½å™¨å·²é—œé–‰ã€‚")
    return pd.DataFrame(all_data)

# --- UI ä»‹é¢ ---
st.title("ğŸ· è–©å…‹æ–¯é¢¨å¹å˜´å¸‚å ´èª¿æŸ¥ (å¯¦æ™‚æ—¥èªŒç‰ˆ)")

url_input = st.text_area("è«‹è¼¸å…¥ç¶²å€ (æ¯è¡Œä¸€å€‹)ï¼š", height=100)
if st.button("â• æ›´æ–°ç›£æ§æ¸…å–®"):
    st.session_state.url_list = [u.strip() for u in url_input.split("\n") if u.strip()]
    st.success("æ¸…å–®å·²æ›´æ–°")

if st.session_state.url_list:
    st.write(f"ç›®å‰ç›£æ§ï¼š{len(st.session_state.url_list)} å€‹ç¶²å€")
    
    if st.button("ğŸš€ é–‹å§‹å…¨æ•¸æ‹”å›"):
        results = scrape_with_live_logs(st.session_state.url_list)
        if not results.empty:
            st.session_state.df_final = results
            st.dataframe(results)

if 'df_final' in st.session_state:
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        st.session_state.df_final.to_excel(writer, index=False)
    st.download_button("ğŸ“¥ ä¸‹è¼‰ Excel å ±å‘Š", output.getvalue(), "sax_report.xlsx")
