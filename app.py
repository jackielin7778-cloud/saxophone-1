import streamlit as st
import pandas as pd
import time
import random
import re
import os
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from io import BytesIO

# --- é é¢é…ç½® ---
st.set_page_config(page_title="è–©å…‹æ–¯é¢¨å¹å˜´å¸‚å ´èª¿æŸ¥ç³»çµ±", layout="wide")

if 'url_list' not in st.session_state:
    st.session_state.url_list = []

def get_driver():
    """å¼·åŒ–ç‰ˆï¼šè‡ªå‹•åµæ¸¬ Streamlit Cloud ç’°å¢ƒè·¯å¾‘"""
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    # å˜—è©¦å¤šå€‹ Linux ä¸‹ Chromium å¯èƒ½çš„å­˜æ”¾è·¯å¾‘
    potential_binary_paths = [
        "/usr/bin/chromium",
        "/usr/bin/chromium-browser",
        "/usr/lib/chromium-browser/chromium-browser"
    ]
    
    for path in potential_binary_paths:
        if os.path.exists(path):
            chrome_options.binary_location = path
            break

    # å˜—è©¦å¤šå€‹ Driver å¯èƒ½çš„è·¯å¾‘
    potential_driver_paths = [
        "/usr/bin/chromedriver",
        "/usr/lib/chromium-browser/chromedriver"
    ]
    
    driver_executable = None
    for path in potential_driver_paths:
        if os.path.exists(path):
            driver_executable = path
            break

    if driver_executable:
        service = Service(driver_executable)
        return webdriver.Chrome(service=service, options=chrome_options)
    else:
        # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œå˜—è©¦è®“ç³»çµ±è‡ªå·±æ‰¾ (æœ€å¾Œæ‰‹æ®µ)
        return webdriver.Chrome(options=chrome_options)

def scrape_data(urls):
    all_data = []
    try:
        driver = get_driver()
    except Exception as e:
        st.error(f"âŒ ç€è¦½å™¨å•Ÿå‹•å¤±æ•—ã€‚é€™é€šå¸¸æ˜¯ Streamlit ç’°å¢ƒå°šæœªå®Œå…¨è£å¥½ packages.txt å°è‡´ã€‚éŒ¯èª¤è©³æƒ…: {e}")
        return pd.DataFrame()

    progress_bar = st.progress(0)
    for index, url in enumerate(urls):
        try:
            driver.get(url)
            time.sleep(random.uniform(5, 8)) 
            page_source = driver.page_source
            
            platform = "è¦çš®" if "shopee" in url else "Yahooæ‹è³£"
            
            # è³£æ–¹åç¨±
            seller_name = "æœªçŸ¥è³£å®¶"
            try:
                if platform == "è¦çš®":
                    seller_name = driver.find_element(By.CSS_SELECTOR, 'span[class*="seller"], ._23_19X').text
                else:
                    seller_name = driver.find_element(By.CSS_SELECTOR, '.name, .seller-name').text
            except:
                seller_name = "éœ€é€²å…¥ç¶²é ç¢ºèª"

            # å”®åƒ¹
            price = "0"
            price_match = re.search(r'\$\s*[0-9,]+', page_source)
            if price_match:
                price = price_match.group()

            # é©ç”¨æ¨‚å™¨
            content = page_source.lower()
            if "alto" in content or "ä¸­éŸ³" in content:
                instrument = "ä¸­éŸ³Alto"
            elif "tenor" in content or "æ¬¡ä¸­éŸ³" in content:
                instrument = "æ¬¡ä¸­éŸ³Tenor"
            elif "soprano" in content or "é«˜éŸ³" in content:
                instrument = "é«˜éŸ³Soprano"
            else:
                instrument = "å…¶ä»–/ä¸é™"

            all_data.append({
                "ä¾†æºå¹³å°": platform,
                "è³£æ–¹åç¨±": seller_name,
                "é©ç”¨æ¨‚å™¨": instrument,
                "å”®åƒ¹": price,
                "å•†å“ç¶²å€": url
            })
        except Exception as e:
            st.warning(f"è·³éç¶²å€: {url}")
        
        progress_bar.progress((index + 1) / len(urls))
    
    driver.quit()
    return pd.DataFrame(all_data)

# --- Streamlit UI ---
st.title("ğŸ· è–©å…‹æ–¯é¢¨å¹å˜´å¸‚å ´èª¿æŸ¥ç³»çµ±")

new_url = st.text_input("è¼¸å…¥å•†å“ç¶²å€ï¼š", key="url_input")
if st.button("â• æ–°å¢ç¶²å€"):
    if new_url and new_url not in st.session_state.url_list:
        st.session_state.url_list.append(new_url)

if st.session_state.url_list:
    st.subheader("ğŸ“‹ ç›£æ§æ¸…å–®")
    for u in st.session_state.url_list:
        st.text(u)

    if st.button("ğŸš€ é–‹å§‹å…¨æ•¸æ‹”å›"):
        results_df = scrape_data(st.session_state.url_list)
        if not results_df.empty:
            st.session_state.last_result = results_df
            st.dataframe(results_df)

    if 'last_result' in st.session_state:
        output = BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            st.session_state.last_result.to_excel(writer, index=False)
        st.download_button("ğŸ“¥ ä¸‹è¼‰ Excel å ±å‘Š", output.getvalue(), "sax_report.xlsx")
