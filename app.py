import streamlit as st
import pandas as pd
import time
import random
import re
import os
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from io import BytesIO

# --- 1. é é¢é…ç½® ---
st.set_page_config(page_title="ğŸ· è–©å…‹æ–¯é¢¨å¹å˜´ï¼šåº—å®¶å°ˆå‘èª¿æŸ¥", layout="wide")

def get_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument(f"--window-size={random.randint(1200, 1600)},{random.randint(800, 1000)}")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    
    ua = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
    chrome_options.add_argument(f"user-agent={ua}")
    
    for path in ["/usr/bin/chromium", "/usr/bin/chromium-browser"]:
        if os.path.exists(path):
            chrome_options.binary_location = path
            break
            
    service = Service("/usr/bin/chromedriver") if os.path.exists("/usr/bin/chromedriver") else Service()
    driver = webdriver.Chrome(service=service, options=chrome_options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    return driver

def scrape_store_search(base_url):
    all_items = []
    log_placeholder = st.empty()
    logs = []

    def log(msg):
        logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")
        log_placeholder.code("\n".join(logs[-10:]))

    # --- é—œéµï¼šå»ºæ§‹åº—å®¶æœå°‹ç¶²å€ ---
    # å¦‚æœç¶²å€å·²ç¶“æœ‰åƒæ•¸ï¼Œç”¨ &p=ï¼Œå¦å‰‡ç”¨ ?p=
    search_query = "å¹å˜´"
    if "?" in base_url:
        target_url = f"{base_url.rstrip('/')}&p={search_query}"
    else:
        target_url = f"{base_url.rstrip('/')}/search/auction/product?p={search_query}"

    try:
        driver = get_driver()
        log(f"ğŸ•µï¸ é€²å…¥åº—å®¶ç¶²å€ä¸¦éæ¿¾ã€Œ{search_query}ã€...")
        driver.get(target_url)
        time.sleep(random.uniform(5, 8))
        
        # æ»¾å‹•åŠ è¼‰
        driver.execute_script("window.scrollTo(0, 1000);")
        time.sleep(2)

        log(f"ğŸ“„ åº—å®¶é é¢æ¨™é¡Œ: {driver.title}")
        
        # ç²å–å•†å“å…ƒç´  (Yahoo åº—å®¶é é¢çµæ§‹)
        # å˜—è©¦å¤šç¨®åº—å®¶å¸¸ç”¨çš„å•†å“å®¹å™¨
        elements = driver.find_elements(By.CSS_SELECTOR, 'li[data-item-id], div[class*="BaseItem"], .item-container')
        
        brand_list = ["Selmer", "Vandoren", "Yanagisawa", "Meyer", "Yamaha", "Otto Link", "Beechler", "JodyJazz"]
        
        for el in elements:
            try:
                # æŠ“å–æ¨™é¡Œèˆ‡åƒ¹æ ¼
                text = el.text.strip().replace("\n", " ")
                if "$" not in text: continue
                
                # æŠ“å–é€£çµ
                link_el = el.find_element(By.TAG_NAME, "a")
                link = link_el.get_attribute("href")
                
                # åƒ¹æ ¼æ­£å‰‡
                p_match = re.search(r'\$\s*[0-9,]+', text)
                price = p_match.group() if p_match else "N/A"
                
                title = text[:80].strip()
                
                # å“ç‰Œè­˜åˆ¥
                brand = "å…¶ä»–"
                for b in brand_list:
                    if b.lower() in title.lower():
                        brand = b
                        break
                
                # æ¨‚å™¨åˆ¤å®š
                instrument = "å…¶ä»–"
                if "alto" in title.lower() or "ä¸­éŸ³" in title.lower(): instrument = "ä¸­éŸ³Alto"
                elif "tenor" in title.lower() or "æ¬¡ä¸­éŸ³" in title.lower(): instrument = "æ¬¡ä¸­éŸ³Tenor"

                all_items.append({
                    "å“ç‰Œ": brand,
                    "å•†å“è³‡è¨Š": title,
                    "é©ç”¨æ¨‚å™¨": instrument,
                    "å”®åƒ¹": price,
                    "ç¶²å€": link
                })
            except: continue

        df = pd.DataFrame(all_items).drop_duplicates(subset=['å•†å“è³‡è¨Š', 'å”®åƒ¹'])
        log(f"âœ… æˆåŠŸå¾åº—å®¶æ‹”å› {len(df)} ç­†ã€Œå¹å˜´ã€ç›¸é—œæ•¸æ“š")
        driver.quit()
        return df
    except Exception as e:
        log(f"âŒ ç•°å¸¸: {str(e)}")
        return pd.DataFrame()

# --- 2. UI ä»‹é¢ ---
st.title("ğŸ· è–©å…‹æ–¯é¢¨å¹å˜´ï¼šç‰¹å®šåº—å®¶å°ˆå‘èª¿æŸ¥")
st.markdown("è¼¸å…¥ **åº—å®¶é¦–é ç¶²å€**ï¼ˆä¾‹å¦‚ï¼š`https://tw.bid.yahoo.com/booth/Y12345678`ï¼‰ï¼Œç³»çµ±æœƒè‡ªå‹•æœå°‹åº—å…§çš„å¹å˜´ã€‚")

# é è¨­ä¸€å€‹ç¤ºä¾‹åº—å®¶ (å”å·éŸ³æ¨‚åœ¨ Yahoo çš„ç¯„ä¾‹è·¯å¾‘çµæ§‹)
default_store = "https://tw.bid.yahoo.com/booth/Y9133606367"
store_url = st.text_input("åº—å®¶ç¶²å€ï¼š", value=default_store)

if st.button("ğŸš€ é–‹å§‹åº—å…§æœç´¢"):
    if store_url:
        results = scrape_store_search(store_url)
        if not results.empty:
            st.session_state.store_df = results
            st.dataframe(results, use_container_width=True)
        else:
            st.warning("åœ¨æ­¤åº—å®¶å…§æ‰¾ä¸åˆ°ç›¸é—œå•†å“ï¼Œæˆ– IP é­æš«æ™‚é˜»æ“‹ã€‚")

if 'store_df' in st.session_state:
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        st.session_state.store_df.to_excel(writer, index=False)
    st.download_button("ğŸ“¥ ä¸‹è¼‰åº—å®¶èª¿æŸ¥ Excel", output.getvalue(), "store_sax_report.xlsx")
