import streamlit as st
import pandas as pd
import time
import random
import re
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from io import BytesIO

# --- é é¢é…ç½® ---
st.set_page_config(page_title="è–©å…‹æ–¯é¢¨å¹å˜´å¸‚å ´èª¿æŸ¥ç³»çµ±", layout="wide")

# åˆå§‹åŒ– session_state
if 'url_list' not in st.session_state:
    st.session_state.url_list = []

def get_driver():
    """å°ˆç‚º Streamlit Cloud ç’°å¢ƒè¨­è¨ˆçš„ Driver å•Ÿå‹•è¨­å®š"""
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    # æŒ‡å‘ Streamlit Cloud ç’°å¢ƒä¸­ Chromium çš„é è¨­è·¯å¾‘
    chrome_options.binary_location = "/usr/bin/chromium"
    
    # å»ºç«‹ Service å°è±¡ï¼ŒæŒ‡å‘ç³»çµ±ç’°å¢ƒä¸­çš„ chromedriver
    service = Service("/usr/bin/chromedriver")
    
    return webdriver.Chrome(service=service, options=chrome_options)

def scrape_data(urls):
    all_data = []
    try:
        driver = get_driver()
    except Exception as e:
        st.error(f"ç€è¦½å™¨å•Ÿå‹•å¤±æ•—ï¼Œè«‹æª¢æŸ¥ packages.txt æ˜¯å¦æ­£ç¢ºå®‰è£ã€‚éŒ¯èª¤è©³æƒ…: {e}")
        return pd.DataFrame()

    progress_bar = st.progress(0)
    for index, url in enumerate(urls):
        try:
            driver.get(url)
            # éš¨æ©Ÿå»¶é²ï¼Œæ¨¡æ“¬çœŸäººè¡Œç‚º
            time.sleep(random.uniform(5, 8)) 
            
            page_source = driver.page_source
            
            # 1. åˆ¤æ–·å¹³å°
            platform = "è¦çš®" if "shopee" in url else "Yahooæ‹è³£"
            
            # 2. è³£æ–¹åç¨± (é€šç”¨æ¨¡ç³Šæœå°‹)
            seller_name = "æœªçŸ¥è³£å®¶"
            try:
                if platform == "è¦çš®":
                    seller_name = driver.find_element(By.CSS_SELECTOR, 'span[class*="seller"], ._23_19X').text
                else:
                    seller_name = driver.find_element(By.CSS_SELECTOR, '.name, .seller-name').text
            except:
                seller_name = "éœ€é€²å…¥ç¶²é ç¢ºèª"

            # 3. å”®åƒ¹è§£æ
            price = "0"
            price_match = re.search(r'\$\s*[0-9,]+', page_source)
            if price_match:
                price = price_match.group()

            # 4. é©ç”¨æ¨‚å™¨ (é—œéµå­—åˆ¤å®š)
            content = page_source.lower()
            if "alto" in content or "ä¸­éŸ³" in content:
                instrument = "ä¸­éŸ³Alto"
            elif "tenor" in content or "æ¬¡ä¸­éŸ³" in content:
                instrument = "æ¬¡ä¸­éŸ³Tenor"
            elif "soprano" in content or "é«˜éŸ³" in content:
                instrument = "é«˜éŸ³Soprano"
            else:
                instrument = "å…¶ä»–/ä¸é™"

            all_data.append({
                "ä¾†æºå¹³å°": platform,
                "è³£æ–¹åç¨±": seller_name,
                "é©ç”¨æ¨‚å™¨": instrument,
                "å”®åƒ¹": price,
                "å•†å“ç¶²å€": url
            })
        except Exception as e:
            st.warning(f"è·³éç„¡æ³•è®€å–çš„ç¶²å€: {url}")
        
        progress_bar.progress((index + 1) / len(urls))
    
    driver.quit()
    return pd.DataFrame(all_data)

# --- å‰å°ç¶²ç«™ä»‹é¢ ---
st.title("ğŸ· è–©å…‹æ–¯é¢¨å¹å˜´å¸‚å ´èª¿æŸ¥ç³»çµ±")

# ç¶²å€è¼¸å…¥åŠŸèƒ½
with st.container():
    new_url = st.text_input("è¼¸å…¥æ–°çš„å•†å“ç¶²å€ (è¦çš®æˆ–Yahoo)ï¼š", key="url_input")
    if st.button("â• æ–°å¢è‡³èª¿æŸ¥æ¸…å–®"):
        if new_url:
            if new_url not in st.session_state.url_list:
                st.session_state.url_list.append(new_url)
                st.success("ç¶²å€å·²æˆåŠŸç´€éŒ„ï¼Œä¸æœƒå› é‡æ–°æ•´ç†è€Œæ¶ˆå¤±ã€‚")
            else:
                st.warning("æ­¤ç¶²å€å·²åœ¨æ¸…å–®ä¸­ã€‚")

# é¡¯ç¤ºç›£æ§æ¸…å–®
if st.session_state.url_list:
    st.divider()
    st.subheader("ğŸ“‹ ç›®å‰ç›£æ§ä¸­çš„ç¶²å€")
    for i, u in enumerate(st.session_state.url_list):
        st.write(f"{i+1}. {u}")

    col1, col2 = st.columns([1, 4])
    with col1:
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºç¶²å€"):
            st.session_state.url_list = []
            st.rerun()
    with col2:
        if st.button("ğŸš€ åŸ·è¡Œå…¨æ•¸æ‹”å› (é–‹å§‹çˆ¬èŸ²)"):
            with st.spinner("æ­£åœ¨æ¨¡æ“¬ç€è¦½å™¨æ“ä½œä¸­..."):
                results_df = scrape_data(st.session_state.url_list)
                if not results_df.empty:
                    st.session_state.last_result = results_df
                    st.dataframe(results_df)

    # åŒ¯å‡ºèˆ‡ä¸‹è¼‰
    if 'last_result' in st.session_state:
        output = BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            st.session_state.last_result.to_excel(writer, index=False, sheet_name='å¹å˜´èª¿æŸ¥')
        
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰ Excel å ±å‘Š",
            data=output.getvalue(),
            file_name="sax_mouthpiece_report.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
