import streamlit as st
import pandas as pd
import time
import random
import re
import os
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from io import BytesIO

# --- é é¢é…ç½® ---
st.set_page_config(page_title="ğŸ· è–©å…‹æ–¯é¢¨å¹å˜´å¸‚å ´èª¿æŸ¥ç³»çµ±", layout="wide")

if 'url_list' not in st.session_state:
    st.session_state.url_list = []

def get_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    # æ›´æ›æ›´åƒçœŸäººçš„ User-Agent
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36")
    
    # è‡ªå‹•åµæ¸¬è·¯å¾‘
    for path in ["/usr/bin/chromium", "/usr/bin/chromium-browser"]:
        if os.path.exists(path):
            chrome_options.binary_location = path
            break
            
    service = Service("/usr/bin/chromedriver") if os.path.exists("/usr/bin/chromedriver") else Service()
    return webdriver.Chrome(service=service, options=chrome_options)

def scrape_data(urls):
    all_data = []
    driver = get_driver()
    wait = WebDriverWait(driver, 15) # æœ€å¤šç­‰ 15 ç§’
    
    progress_bar = st.progress(0)
    for index, url in enumerate(urls):
        try:
            driver.get(url)
            # æ¨¡æ“¬çœŸäººæ»¾å‹•é é¢ï¼Œè§¸ç™¼å‹•æ…‹è¼‰å…¥
            driver.execute_script("window.scrollTo(0, 500);")
            time.sleep(random.uniform(5, 8)) 
            
            # --- åˆ¤æ–·å¹³å°ä¸¦æŠ“å– ---
            platform = "è¦çš®" if "shopee" in url else "Yahooæ‹è³£"
            seller_name = "æœªçŸ¥è³£å®¶"
            price = "0"
            
            if platform == "è¦çš®":
                try:
                    # è¦çš®è³£å®¶åç¨±å¯èƒ½åœ¨ä¸åŒçš„ Class è£¡
                    seller_el = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div.V67tSj, ._23_19X, .v_67_Sj')))
                    seller_name = seller_el.text
                    # è¦çš®åƒ¹æ ¼
                    price_el = driver.find_element(By.CSS_SELECTOR, 'div.pqm66z, .G277_P')
                    price = price_el.text
                except:
                    seller_name = "åµæ¸¬åˆ°é˜»æ“‹(éœ€äººå·¥)"
            else:
                # Yahoo æ‹è³£
                try:
                    seller_el = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.seller-name, a[data-curst]')))
                    seller_name = seller_el.text
                    price_el = driver.find_element(By.CSS_SELECTOR, '.price, .product-price')
                    price = price_el.text
                except:
                    seller_name = "Yahooè§£æå¤±æ•—"

            # --- é©ç”¨æ¨‚å™¨åˆ¤å®š ---
            page_content = driver.page_source.lower()
            if any(k in page_content for k in ["alto", "ä¸­éŸ³"]):
                instrument = "ä¸­éŸ³Alto"
            elif any(k in page_content for k in ["tenor", "æ¬¡ä¸­éŸ³"]):
                instrument = "æ¬¡ä¸­éŸ³Tenor"
            elif any(k in page_content for k in ["soprano", "é«˜éŸ³"]):
                instrument = "é«˜éŸ³Soprano"
            else:
                instrument = "å…¶ä»–/é€šç”¨"

            all_data.append({
                "ä¾†æºå¹³å°": platform,
                "è³£æ–¹åç¨±": seller_name,
                "é©ç”¨æ¨‚å™¨": instrument,
                "å”®åƒ¹": price,
                "ç¶²å€": url
            })
        except Exception as e:
            st.error(f"è§£æ {url} æ™‚ç™¼ç”ŸéŒ¯èª¤")
        
        progress_bar.progress((index + 1) / len(urls))
    
    driver.quit()
    return pd.DataFrame(all_data)

# --- Streamlit UI ---
st.title("ğŸ· è–©å…‹æ–¯é¢¨å¹å˜´å¸‚å ´èª¿æŸ¥ç³»çµ±")

with st.form("input_form"):
    url_to_add = st.text_input("è¼¸å…¥å•†å“ç¶²å€ï¼š")
    add_btn = st.form_submit_button("æ–°å¢ç¶²å€")
    if add_btn and url_to_add:
        if url_to_add not in st.session_state.url_list:
            st.session_state.url_list.append(url_to_add)
            st.success(f"å·²åŠ å…¥ï¼ç›®å‰å…±æœ‰ {len(st.session_state.url_list)} å€‹ç¶²å€")

if st.session_state.url_list:
    st.write("ğŸ“‹ å¾…çˆ¬å–æ¸…å–®ï¼š")
    st.info("\n".join(st.session_state.url_list))
    
    if st.button("ğŸš€ é–‹å§‹å…¨æ•¸æ‹”å›"):
        with st.spinner("æ­£åœ¨åˆ†æç¶²é çµæ§‹ï¼Œè«‹ç¨å€™..."):
            df = scrape_data(st.session_state.url_list)
            if not df.empty:
                st.session_state.df_result = df
                st.dataframe(df)
            else:
                st.error("âš ï¸ æŠ“å–ä¸åˆ°è³‡æ–™ï¼Œå¯èƒ½æ˜¯è¢«ç¶²ç«™é˜²ç«ç‰†å°é–äº† IPã€‚")

    if 'df_result' in st.session_state:
        output = BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            st.session_state.df_result.to_excel(writer, index=False)
        st.download_button("ğŸ“¥ ä¸‹è¼‰ Excel èª¿æŸ¥å ±è¡¨", output.getvalue(), "sax_report.xlsx")

    if st.button("ğŸ—‘ï¸ æ¸…ç©ºç´€éŒ„"):
        st.session_state.url_list = []
        if 'df_result' in st.session_state: del st.session_state.df_result
        st.rerun()
