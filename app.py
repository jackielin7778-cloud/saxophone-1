import streamlit as st
import pandas as pd
import time
import random
import re
import os
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from io import BytesIO

st.set_page_config(page_title="ğŸ· å¹å˜´èª¿æŸ¥ï¼šé›²ç«¯ç”Ÿå­˜ç‰ˆ", layout="wide")

def get_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless=new") # ä½¿ç”¨æœ€æ–°çš„ç„¡é ­æ¨¡å¼ï¼Œæ›´æ¥è¿‘çœŸæ©Ÿ
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    
    # --- æ ¸å¿ƒå½è£ï¼šæŠ¹é™¤è‡ªå‹•åŒ–ç‰¹å¾µ ---
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    # å½è£ UAï¼šä½¿ç”¨ä¸€å€‹éå¸¸å…·é«”çš„çœŸå¯¦ç‰ˆæœ¬
    ua = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
    chrome_options.add_argument(f"user-agent={ua}")
    
    # è¨­å®šä¸€å€‹è¼ƒå¤§çš„è¦–çª—ï¼Œé˜²æ­¢ Lazy Load åˆ¤å®š
    chrome_options.add_argument("--window-size=1920,1080")

    for path in ["/usr/bin/chromium", "/usr/bin/chromium-browser"]:
        if os.path.exists(path):
            chrome_options.binary_location = path
            break
            
    service = Service("/usr/bin/chromedriver") if os.path.exists("/usr/bin/chromedriver") else Service()
    driver = webdriver.Chrome(service=service, options=chrome_options)

    # --- JavaScript æ³¨å…¥ï¼šæ·±åº¦æŠ¹é™¤æŒ‡ç´‹ ---
    driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
        "source": """
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
            window.chrome = {
                runtime: {}
            };
            Object.defineProperty(navigator, 'languages', {
                get: () => ['zh-TW', 'zh']
            });
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5]
            });
        """
    })
    return driver

def scrape_cloud_final_attempt(base_url):
    all_items = []
    log_placeholder = st.empty()
    logs = []

    def log(msg):
        logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")
        log_placeholder.code("\n".join(logs[-8:]))

    # ç¢ºä¿æœå°‹è·¯å¾‘æ­£ç¢º
    clean_url = base_url.split('?')[0].rstrip('/')
    target_url = f"{clean_url}/search/auction/product?p=å¹å˜´"

    try:
        driver = get_driver()
        log(f"ğŸ•µï¸ æ­£åœ¨å˜—è©¦ç©¿é€ Yahoo é˜²ç«ç‰†...")
        
        # å½è£ï¼šå…ˆå» Google å†å» Yahoo (Referrer å½è£)
        driver.get("https://www.google.com")
        time.sleep(2)
        
        driver.get(target_url)
        
        # å¢åŠ éš¨æ©Ÿç­‰å¾…ï¼Œé¿å…è¢«ç™¼ç¾æ˜¯å›ºå®šé »ç‡
        wait_time = random.randint(15, 25)
        log(f"â³ éœå€™æ•¸æ“šæ¸²æŸ“ä¸­ ({wait_time}s)...")
        time.sleep(wait_time)

        # æš´åŠ›æ»¾å‹•
        driver.execute_script("window.scrollTo(0, 800);")
        time.sleep(2)

        source = driver.page_source
        log(f"ğŸ“¦ åŸå§‹ç¢¼é•·åº¦: {len(source)} å­—å…ƒ")

        # åˆ¤å®šæ˜¯å¦æˆåŠŸå–å¾—å…§å®¹
        if len(source) < 50000:
            log("âš ï¸ è­¦å‘Šï¼šå…§å®¹é•·åº¦ç•°å¸¸ï¼Œå¯èƒ½ä»è¢«é˜»æ“‹ã€‚")
        
        # å°‹æ‰¾æ‰€æœ‰å•†å“å®¹å™¨
        # åº—å®¶æœå°‹çµæœé é¢çš„å•†å“é€šå¸¸åœ¨ div.GridItem__gridItem___ æˆ–é¡ä¼¼æ¨™ç±¤
        containers = driver.find_elements(By.CSS_SELECTOR, 'div[class*="Item__itemContainer"], li[data-item-id], [class*="BaseItem"]')
        
        log(f"ğŸ” æ‰¾åˆ° {len(containers)} å€‹å•†å“å¡Š")

        brand_list = ["Selmer", "Vandoren", "Yanagisawa", "Meyer", "Yamaha", "Otto Link", "Beechler", "JodyJazz"]

        for el in containers:
            try:
                title = el.find_element(By.CSS_SELECTOR, '[class*="ItemName"]').text
                price = el.find_element(By.CSS_SELECTOR, '[class*="ItemPrice"]').text
                
                brand = "å…¶ä»–"
                for b in brand_list:
                    if b.lower() in title.lower():
                        brand = b
                        break
                
                all_items.append({
                    "å“ç‰Œ": brand,
                    "å•†å“è³‡è¨Š": title,
                    "å”®åƒ¹": price,
                    "ç¶²å€": target_url
                })
            except: continue

        df = pd.DataFrame(all_items).drop_duplicates(subset=['å•†å“è³‡è¨Š'])
        log(f"âœ… æˆåŠŸæå– {len(df)} ç­†æ•¸æ“š")
        driver.quit()
        return df
    except Exception as e:
        log(f"âŒ ç•°å¸¸: {str(e)}")
        if 'driver' in locals(): driver.quit()
        return pd.DataFrame()

# UI ä»‹é¢
st.title("ğŸ· å¹å˜´èª¿æŸ¥ï¼šé›²ç«¯ç”Ÿå­˜ç‰ˆ")
store_url = st.text_input("åº—å®¶ç¶²å€ï¼š", value="https://tw.bid.yahoo.com/booth/Y9133606367")

if st.button("ğŸš€ å•Ÿå‹•èª¿æŸ¥"):
    results = scrape_cloud_final_attempt(store_url)
    if not results.empty:
        st.dataframe(results, use_container_width=True)
        # æä¾›ä¸‹è¼‰
        output = BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            results.to_excel(writer, index=False)
        st.download_button("ğŸ“¥ ä¸‹è¼‰å ±å‘Š", output.getvalue(), "sax_report.xlsx")
    else:
        st.error("ç›®å‰é›²ç«¯ IP é­ Yahoo å°é–ï¼Œè«‹ç¨å€™å†è©¦ã€‚")
