import streamlit as st
import pandas as pd
import time
import random
import re
import os
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from io import BytesIO

# --- 1. é é¢é…ç½® ---
st.set_page_config(page_title="ğŸ· è–©å…‹æ–¯é¢¨å¹å˜´ï¼šåº—å®¶å°ˆå‘èª¿æŸ¥", layout="wide")

def get_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument(f"--window-size=1920,1080")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    
    ua = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
    chrome_options.add_argument(f"user-agent={ua}")
    
    for path in ["/usr/bin/chromium", "/usr/bin/chromium-browser"]:
        if os.path.exists(path):
            chrome_options.binary_location = path
            break
            
    service = Service("/usr/bin/chromedriver") if os.path.exists("/usr/bin/chromedriver") else Service()
    driver = webdriver.Chrome(service=service, options=chrome_options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    return driver

def scrape_store_search(base_url):
    all_items = []
    log_placeholder = st.empty()
    logs = []

    def log(msg):
        logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")
        log_placeholder.code("\n".join(logs[-10:]))

    # --- ä¿®æ­£æœå°‹ URL å»ºæ§‹é‚è¼¯ ---
    search_query = "å¹å˜´"
    # æ¸…ç†ç¶²å€ï¼Œç¢ºä¿è·¯å¾‘æ­£ç¢º
    base_url = base_url.split('?')[0].rstrip('/')
    # Yahoo åº—å…§æœå°‹çš„æ¨™æº–æ ¼å¼
    target_url = f"{base_url}/search/auction/product?p={search_query}"

    try:
        driver = get_driver()
        log(f"ğŸ•µï¸ æ­£åœ¨æ½›å…¥åº—å®¶æœå°‹é é¢: {target_url}")
        driver.get(target_url)
        
        # å¢åŠ ç­‰å¾…æ™‚é–“ï¼Œç¢ºä¿åº—å®¶é é¢çš„å‹•æ…‹çµ„ä»¶è¼‰å…¥
        time.sleep(10)
        
        # åŸ·è¡Œå¤šæ¬¡å¾®å¹…æ»¾å‹•ï¼Œè§¸ç™¼ Lazy Load
        for _ in range(3):
            driver.execute_script("window.scrollBy(0, 500);")
            time.sleep(1)

        log(f"ğŸ“„ æ¨™é¡Œç¢ºèª: {driver.title}")
        
        # --- é‡å°åº—å®¶é é¢ (Booth) çš„å¤šé‡æ¢é‡ ---
        # 1. å˜—è©¦æŠ“å–æ‰€æœ‰å•†å“å¡ç‰‡
        items = driver.find_elements(By.CSS_SELECTOR, 'div[class*="Item__itemContainer"], .item-container, li[data-item-id]')
        
        # 2. å¦‚æœæ²’æŠ“åˆ°ï¼Œå˜—è©¦æ›´å»£æ³›çš„ A æ¨™ç±¤ (å•†å“é€£çµ)
        if not items:
            log("âš ï¸ æ¨™ç±¤æ¢é‡å¤±æ•ˆï¼Œå˜—è©¦æ·±åº¦éæ­·å•†å“ç¯€é»...")
            items = driver.find_elements(By.XPATH, "//div[contains(@class, 'ProductCard')] | //div[contains(@class, 'BaseItem')]")

        log(f"ğŸ“¦ åµæ¸¬åˆ° {len(items)} å€‹å•†å“å€å¡Š")

        brand_list = ["Selmer", "Vandoren", "Yanagisawa", "Meyer", "Yamaha", "Otto Link", "Beechler", "JodyJazz"]
        
        for item in items:
            try:
                raw_text = item.text.replace("\n", " ").strip()
                if "$" not in raw_text: continue
                
                # æå–æ¨™é¡Œèˆ‡åƒ¹æ ¼
                # åº—å®¶é é¢é€šå¸¸æ¨™é¡Œåœ¨ a æ¨™ç±¤å…§
                try:
                    title_el = item.find_element(By.CSS_SELECTOR, 'span[class*="ItemName"], a[class*="ItemName"]')
                    title = title_el.text
                    link = title_el.find_element(By.XPATH, "./ancestor::a").get_attribute("href")
                except:
                    title = raw_text[:60]
                    link = target_url # ä¿åº•
                
                p_match = re.search(r'\$\s*[0-9,]+', raw_text)
                price = p_match.group() if p_match else "N/A"
                
                # å“ç‰Œèˆ‡æ¨‚å™¨åˆ¤å®š
                brand = "å…¶ä»–"
                for b in brand_list:
                    if b.lower() in title.lower():
                        brand = b
                        break
                
                instrument = "å…¶ä»–"
                if "alto" in title.lower() or "ä¸­éŸ³" in title.lower(): instrument = "ä¸­éŸ³Alto"
                elif "tenor" in title.lower() or "æ¬¡ä¸­éŸ³" in title.lower(): instrument = "æ¬¡ä¸­éŸ³Tenor"

                all_items.append({
                    "å“ç‰Œ": brand,
                    "å•†å“è³‡è¨Š": title,
                    "é©ç”¨æ¨‚å™¨": instrument,
                    "å”®åƒ¹": price,
                    "ç¶²å€": link
                })
            except: continue

        df = pd.DataFrame(all_items).drop_duplicates(subset=['å•†å“è³‡è¨Š'])
        log(f"âœ… èª¿æŸ¥å®Œæˆï¼Œå…±æ‹”å› {len(df)} ç­†æ•¸æ“š")
        driver.quit()
        return df
    except Exception as e:
        log(f"âŒ ç•°å¸¸: {str(e)}")
        if 'driver' in locals(): driver.quit()
        return pd.DataFrame()

# --- UI ä»‹é¢ ---
st.title("ğŸ· è–©å…‹æ–¯é¢¨å¹å˜´ï¼šç‰¹å®šåº—å®¶èª¿æŸ¥ç³»çµ±")
store_url = st.text_input("è«‹è¼¸å…¥åº—å®¶é¦–é ç¶²å€ï¼š", value="https://tw.bid.yahoo.com/booth/Y9133606367")

if st.button("ğŸš€ åŸ·è¡Œåº—å…§å®šå‘èª¿æŸ¥"):
    if store_url:
        results = scrape_store_search(store_url)
        if not results.empty:
            st.session_state.booth_df = results
            st.dataframe(results, use_container_width=True)
        else:
            st.error("æ‰¾ä¸åˆ°å•†å“ã€‚è«‹ç¢ºèªè©²åº—å®¶æ˜¯å¦æœ‰ã€å¹å˜´ã€é—œéµå­—å•†å“ï¼Œæˆ–å˜—è©¦æ›´æ›åº—å®¶ç¶²å€ã€‚")

if 'booth_df' in st.session_state:
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        st.session_state.booth_df.to_excel(writer, index=False)
    st.download_button("ğŸ“¥ ä¸‹è¼‰ Excel å ±å‘Š", output.getvalue(), "store_report.xlsx")
