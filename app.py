import streamlit as st
import pandas as pd
import time
import random
import re
import os
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from io import BytesIO

# --- 1. é é¢åˆå§‹åŒ–èˆ‡ç’°å¢ƒæª¢æŸ¥ ---
st.set_page_config(page_title="ğŸ· è–©å…‹æ–¯é¢¨å¹å˜´å¸‚èª¿ç³»çµ±", layout="wide")

if 'url_list' not in st.session_state:
    st.session_state.url_list = []

def get_driver():
    """å»ºç«‹å…·å‚™éš±èº«ç‰¹å¾µçš„ç€è¦½å™¨"""
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # é›²ç«¯åŸ·è¡Œå¿…é ˆ
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    
    # é—œéµï¼šéš±è— Selenium çš„è‡ªå‹•åŒ–ç‰¹å¾µ
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    # éš¨æ©Ÿ User-Agent é˜²æ­¢è¢«ç›´æ¥è­˜åˆ¥ç‚ºæ©Ÿæˆ¿æ©Ÿå™¨äºº
    ua_list = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    ]
    chrome_options.add_argument(f"user-agent={random.choice(ua_list)}")
    
    # åµæ¸¬ Chromium è·¯å¾‘
    for path in ["/usr/bin/chromium", "/usr/bin/chromium-browser"]:
        if os.path.exists(path):
            chrome_options.binary_location = path
            break
            
    service = Service("/usr/bin/chromedriver") if os.path.exists("/usr/bin/chromedriver") else Service()
    driver = webdriver.Chrome(service=service, options=chrome_options)
    
    # åŸ·è¡Œè…³æœ¬ä»¥ç§»é™¤ webdriver å±¬æ€§
    driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
        "source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
    })
    return driver

def scrape_engine(urls):
    """æ ¸å¿ƒçˆ¬èŸ²å¼•æ“"""
    all_data = []
    log_placeholder = st.empty()
    logs = []

    def log(msg):
        logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")
        log_placeholder.code("\n".join(logs[-10:])) # åªé¡¯ç¤ºæœ€å¾Œ 10 æ¢

    log("ğŸš€ ç€è¦½å™¨å•Ÿå‹•ä¸­...")
    try:
        driver = get_driver()
        # --- é ç†± Session (é˜²æ­¢ç›´æ¥è·³è½‰ç™»å…¥) ---
        log("ğŸ› ï¸ æ­£åœ¨åˆå§‹åŒ– Sessionï¼Œæ¨¡æ“¬æ­£å¸¸ç”¨æˆ¶è¨ªå•...")
        driver.get("https://www.google.com")
        time.sleep(2)
        driver.get("https://shopee.tw/")
        time.sleep(3)
    except Exception as e:
        log(f"âŒ é©…å‹•ç¨‹å¼ç•°å¸¸: {str(e)}")
        return pd.DataFrame()

    progress_bar = st.progress(0)
    for index, url in enumerate(urls):
        try:
            log(f"ğŸ” æ­£åœ¨æŠ“å– ({index+1}/{len(urls)}): {url[:40]}...")
            
            # å½è£ä¾†æºç¶²é 
            driver.execute_cdp_cmd('Network.setExtraHTTPHeaders', {'headers': {'Referer': 'https://www.google.com/'}})
            driver.get(url)
            
            # éš¨æ©Ÿåœç•™ï¼Œæ¨¡æ“¬é–±è®€
            load_time = random.uniform(5, 10)
            log(f"â³ ç­‰å¾…æ¸²æŸ“ {load_time:.1f} ç§’...")
            time.sleep(load_time)
            
            # æª¢æŸ¥çµæœ
            title = driver.title
            log(f"ğŸ“„ ç¶²é æ¨™é¡Œ: {title}")
            
            if "ç™»å…¥" in title or "Login" in title:
                log("âš ï¸ é‡åˆ°ç™»å…¥ç‰†ï¼Œè«‹å˜—è©¦æ‰‹å‹•è¼¸å…¥å…·é«”å•†å“ç¶²å€ã€‚")

            # --- æ•¸æ“šè§£æé‚è¼¯ ---
            platform = "è¦çš®" if "shopee" in url else "Yahooæ‹è³£"
            seller = "æœªçŸ¥è³£å®¶"
            price = "å°šæœªæ“·å–"
            
            # 1. æŠ“å–è³£å®¶
            try:
                if platform == "è¦çš®":
                    # è¦çš®è³£å®¶å¤šç¨® CSS é¸å–å™¨å˜—è©¦
                    s_el = driver.find_elements(By.CSS_SELECTOR, 'span.V67tSj, ._23_19X, .official-shop-label__name')
                    seller = s_el[0].text if s_el else "åµæ¸¬ä¸åˆ°è³£å®¶"
                else:
                    s_el = driver.find_elements(By.CSS_SELECTOR, '.seller-name, a[data-curst]')
                    seller = s_el[0].text if s_el else "åµæ¸¬ä¸åˆ°è³£å®¶"
            except: pass

            # 2. æŠ“å–åƒ¹æ ¼
            try:
                price_match = re.search(r'\$\s*[0-9,]+', driver.page_source)
                price = price_match.group() if price_match else "éœ€é€²å…¥ç¶²é çœ‹"
            except: pass

            # 3. æ¨‚å™¨åˆ¤å®š
            content = driver.page_source.lower()
            instrument = "å…¶ä»–/é€šç”¨"
            if any(k in content for k in ["alto", "ä¸­éŸ³"]): instrument = "ä¸­éŸ³Alto"
            elif any(k in content for k in ["tenor", "æ¬¡ä¸­éŸ³"]): instrument = "æ¬¡ä¸­éŸ³Tenor"
            elif any(k in content for k in ["soprano", "é«˜éŸ³"]): instrument = "é«˜éŸ³Soprano"

            all_data.append({
                "è³£æ–¹åç¨±": seller,
                "é©ç”¨æ¨‚å™¨": instrument,
                "å”®åƒ¹": price,
                "ä¾†æºå¹³å°": platform,
                "å•†å“ç¶²å€": url
            })
            log(f"âœ… å®Œæˆè§£æ: {seller} | {instrument}")

        except Exception as e:
            log(f"âŒ å–®ç­†è§£æéŒ¯èª¤: {str(e)}")
        
        progress_bar.progress((index + 1) / len(urls))

    driver.quit()
    log("ğŸ èª¿æŸ¥ä»»å‹™çµæŸï¼Œç€è¦½å™¨å·²å®‰å…¨é—œé–‰ã€‚")
    return pd.DataFrame(all_data)

# --- 2. å‰å°ä»‹é¢è¨­è¨ˆ ---
st.title("ğŸ· è–©å…‹æ–¯é¢¨å¹å˜´å¸‚å ´èª¿æŸ¥å·¥å…·")
st.markdown("---")

col1, col2 = st.columns([1, 1])

with col1:
    st.subheader("ğŸ“¥ ç¶²å€ç®¡ç†")
    url_input = st.text_area("è«‹è¼¸å…¥å¹å˜´å•†å“ç¶²å€ï¼ˆæ¯è¡Œä¸€å€‹ï¼‰ï¼š", placeholder="https://shopee.tw/product/...", height=150)
    
    col_btn1, col_btn2 = st.columns(2)
    with col_btn1:
        if st.button("â• æ›´æ–°å¾…çˆ¬æ¸…å–®"):
            new_urls = [u.strip() for u in url_input.split("\n") if u.strip()]
            for u in new_urls:
                if u not in st.session_state.url_list:
                    st.session_state.url_list.append(u)
            st.rerun()
    with col_btn2:
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰æ¸…å–®"):
            st.session_state.url_list = []
            if 'df_results' in st.session_state: del st.session_state.df_results
            st.rerun()

    if st.session_state.url_list:
        st.info(f"ç›®å‰æ¸…å–®ä¸­å…±æœ‰ {len(st.session_state.url_list)} å€‹å°è±¡")
        with st.expander("æŸ¥çœ‹æ¸…å–®ç´°ç¯€"):
            st.write(st.session_state.url_list)

with col2:
    st.subheader("ğŸ“‹ é‹ä½œç‹€æ…‹ (Logs)")
    # åŸ·è¡Œçˆ¬èŸ²
    if st.session_state.url_list:
        if st.button("ğŸš€ é–‹å§‹å…¨æ•¸æ‹”å›"):
            final_df = scrape_engine(st.session_state.url_list)
            if not final_df.empty:
                st.session_state.df_results = final_df

# --- 3. çµæœå±•ç¤ºèˆ‡ä¸‹è¼‰ ---
if 'df_results' in st.session_state:
    st.markdown("---")
    st.subheader("ğŸ“Š èª¿æŸ¥çµæœé è¦½")
    st.dataframe(st.session_state.df_results, use_container_width=True)
    
    # Excel ä¸‹è¼‰
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        st.session_state.df_results.to_excel(writer, index=False, sheet_name='Sax_Survey')
    
    st.download_button(
        label="ğŸ“¥ ä¸‹è¼‰ Excel èª¿æŸ¥å ±å‘Š",
        data=output.getvalue(),
        file_name=f"sax_survey_{datetime.now().strftime('%m%d')}.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )

st.markdown("---")
st.caption("èª¿æŸ¥å“¡æç¤ºï¼šè‹¥è¦çš®é »ç¹å‡ºç¾ç™»å…¥è¦æ±‚ï¼Œè«‹å˜—è©¦æ¸›å°‘åŒæ™‚çˆ¬å–çš„ç¶²å€æ•¸é‡ï¼Œæˆ–å»¶é•·ç­‰å¾…æ™‚é–“ã€‚")
